{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debug Evaluation - Per-Check Details\n",
        "\n",
        "**Debugging only.** Runs evaluation logic locally using output_test_pos and reference folders.\n",
        "Uses `log_evaluation()` so the JSON includes per-check details.\n",
        "\n",
        "Adjust `OUTPUT_DIR` and `REFERENCE_DIR` paths as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4a60926f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root for imports (agenthle-base)\n",
        "cwd = Path.cwd()\n",
        "project_root = cwd\n",
        "while project_root != project_root.parent and not (project_root / \"pyproject.toml\").exists():\n",
        "    project_root = project_root.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from utils.evaluation import EvaluationContext\n",
        "\n",
        "# Import NC evaluation helper from main\n",
        "import importlib.util\n",
        "task_dir = cwd if (cwd / \"main.py\").exists() else project_root / \"tasks\" / \"earth_science_tasks\" / \"GEE_downloading_terraclimate_conus\"\n",
        "main_path = task_dir / \"main.py\"\n",
        "spec = importlib.util.spec_from_file_location(\"task_main\", main_path)\n",
        "task_main = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(task_main)\n",
        "_evaluate_netcdf_content = task_main._evaluate_netcdf_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cd650cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONFIG: Adjust paths to your local output/reference folders ===\n",
        "TASK_DIR = task_dir  # from above; or set explicitly\n",
        "OUTPUT_DIR = TASK_DIR / \"output_test_pos\"   # or output_test_neg, output\n",
        "REFERENCE_DIR = TASK_DIR / \"reference\"\n",
        "REQUIRED_VARS = [\"pr\"]\n",
        "TASK_TAG = \"GEE_downloading_terraclimate_conus\"\n",
        "OUTPUT_JSON_DIR = project_root / \"trycua\" / \"cua-bench\" / \"debug\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "87be797d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "  ✓ tif_files_match: TIF count 60 vs ref 60, names match: True\n",
            "  ✓ nc_file_exists: Found 1 NC file(s)\n",
            "  ✓ nc_dims_xy_time: Has x, y, time dims: {'time': 60, 'y': 292, 'x': 670}\n",
            "  ✓ nc_dim_lengths_match_reference: All dim lengths match reference\n",
            "  ✓ nc_variables_match_required: Variables ['pr'] match required ['pr']\n",
            "  ✓ nc_output_minus_reference_zero: output - reference is zero\n",
            "\n",
            "Final score: 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/huiqi/Documents/research/agenthle-base/tasks/earth_science_tasks/rs_001_terraclimate_conus/main.py:223: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  ref_dims = dict(ref_ds.dims)\n",
            "/Users/huiqi/Documents/research/agenthle-base/tasks/earth_science_tasks/rs_001_terraclimate_conus/main.py:227: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  out_dims = dict(out_ds.dims)\n"
          ]
        }
      ],
      "source": [
        "async def run_debug_eval():\n",
        "    \"\"\"Run evaluation with log_evaluation for each check - saves per-check details to JSON.\"\"\"\n",
        "    output_dir = str(OUTPUT_DIR)\n",
        "    reference_dir = str(REFERENCE_DIR)\n",
        "\n",
        "    OUTPUT_JSON_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    async with EvaluationContext(\n",
        "        task_tag=TASK_TAG,\n",
        "        mode=\"custom\",\n",
        "        split=\"train\",\n",
        "        output_dir=str(OUTPUT_JSON_DIR),\n",
        "        auto_save=True,\n",
        "    ) as ctx:\n",
        "        checks = []\n",
        "\n",
        "        try:\n",
        "            output_files = os.listdir(output_dir)\n",
        "            reference_files = os.listdir(reference_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error listing directories: {e}\")\n",
        "            ctx.log_evaluation(identifier=\"list_dirs\", score=0.0, error=str(e), message=\"Failed to list output/reference\")\n",
        "            ctx.add_score(0.0)\n",
        "            return ctx.get_final_score(num_items=1)\n",
        "\n",
        "        output_tif = sorted([f for f in output_files if f.lower().endswith(\".tif\")])\n",
        "        reference_tif = sorted([f for f in reference_files if f.lower().endswith(\".tif\")])\n",
        "\n",
        "        # 1) TIF check\n",
        "        tif_same_length = len(output_tif) == len(reference_tif)\n",
        "        tif_same_names = set(output_tif) == set(reference_tif)\n",
        "        tif_passed = tif_same_length and tif_same_names\n",
        "        msg = f\"TIF count {len(output_tif)} vs ref {len(reference_tif)}, names match: {tif_same_names}\"\n",
        "        ctx.log_evaluation(identifier=\"tif_files_match\", score=1.0 if tif_passed else 0.0, message=msg)\n",
        "        ctx.add_score(1.0 if tif_passed else 0.0)\n",
        "        checks.append({\"check\": \"tif_files_match\", \"passed\": tif_passed, \"message\": msg})\n",
        "\n",
        "        # 2) NC file exists\n",
        "        nc_files = [f for f in output_files if f.lower().endswith(\".nc\")]\n",
        "        nc_exists = len(nc_files) > 0\n",
        "        msg = f\"Found {len(nc_files)} NC file(s)\" if nc_exists else \"No .nc file in output\"\n",
        "        ctx.log_evaluation(identifier=\"nc_file_exists\", score=1.0 if nc_exists else 0.0, message=msg)\n",
        "        ctx.add_score(1.0 if nc_exists else 0.0)\n",
        "        checks.append({\"check\": \"nc_file_exists\", \"passed\": nc_exists, \"message\": msg})\n",
        "\n",
        "        if not nc_exists:\n",
        "            print(\"\\nEvaluation Results:\")\n",
        "            for c in checks:\n",
        "                status = \"✓\" if c[\"passed\"] else \"✗\"\n",
        "                print(f\"  {status} {c['check']}: {c['message']}\")\n",
        "            return ctx.get_final_score(num_items=len(checks))\n",
        "\n",
        "        ref_nc_files = [f for f in reference_files if f.lower().endswith(\".nc\")]\n",
        "        if not ref_nc_files:\n",
        "            ctx.log_evaluation(identifier=\"nc_ref\", score=0.0, error=\"No reference NC file found\")\n",
        "            return ctx.get_final_score(num_items=len(checks))\n",
        "\n",
        "        output_nc_path = str(OUTPUT_DIR / nc_files[0])\n",
        "        reference_nc_path = str(REFERENCE_DIR / ref_nc_files[0])\n",
        "\n",
        "        try:\n",
        "            nc_checks = _evaluate_netcdf_content(output_nc_path, reference_nc_path, REQUIRED_VARS)\n",
        "        except Exception as e:\n",
        "            ctx.log_evaluation(identifier=\"nc_content\", score=0.0, error=str(e))\n",
        "            return ctx.get_final_score(num_items=len(checks))\n",
        "\n",
        "        for c in nc_checks:\n",
        "            ctx.log_evaluation(\n",
        "                identifier=c[\"check\"],\n",
        "                score=1.0 if c[\"passed\"] else 0.0,\n",
        "                message=c[\"message\"],\n",
        "            )\n",
        "            ctx.add_score(1.0 if c[\"passed\"] else 0.0)\n",
        "        checks.extend(nc_checks)\n",
        "\n",
        "        print(\"\\nEvaluation Results:\")\n",
        "        for c in checks:\n",
        "            status = \"✓\" if c[\"passed\"] else \"✗\"\n",
        "            print(f\"  {status} {c['check']}: {c['message']}\")\n",
        "\n",
        "        num_checks = len(checks)\n",
        "        return ctx.get_final_score(num_items=num_checks)\n",
        "\n",
        "\n",
        "# In Jupyter, use await (not asyncio.run - notebook already has an event loop)\n",
        "score = await run_debug_eval()\n",
        "print(f\"\\nFinal score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View saved JSON with per-check details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest: /Users/huiqi/Documents/research/agenthle-base/trycua/cua-bench/debug/rs_001_terraclimate_conus_evaluation_20260213_083648.json\n",
            "{\n",
            "  \"mode\": \"custom\",\n",
            "  \"task_tag\": \"rs_001_terraclimate_conus\",\n",
            "  \"timestamp\": \"2026-02-13T08:36:40.179658\",\n",
            "  \"evaluations\": [\n",
            "    {\n",
            "      \"identifier\": \"tif_files_match\",\n",
            "      \"score\": 1.0,\n",
            "      \"message\": \"TIF count 60 vs ref 60, names match: True\"\n",
            "    },\n",
            "    {\n",
            "      \"identifier\": \"nc_file_exists\",\n",
            "      \"score\": 1.0,\n",
            "      \"message\": \"Found 1 NC file(s)\"\n",
            "    },\n",
            "    {\n",
            "      \"identifier\": \"nc_dims_xy_time\",\n",
            "      \"score\": 1.0,\n",
            "      \"message\": \"Has x, y, time dims: {'time': 60, 'y': 292, 'x': 670}\"\n",
            "    },\n",
            "    {\n",
            "      \"identifier\": \"nc_dim_lengths_match_reference\",\n",
            "      \"score\": 1.0,\n",
            "      \"message\": \"All dim lengths match reference\"\n",
            "    },\n",
            "    {\n",
            "      \"identifier\": \"nc_variables_match_required\",\n",
            "      \"score\": 1.0,\n",
            "      \"message\": \"Variables ['pr'] match required ['pr']\"\n",
            "    },\n",
            "    {\n",
            "      \"identifier\": \"nc_output_minus_reference_zero\",\n",
            "      \"score\": 1.0,\n",
            "      \"message\": \"output - reference is zero\"\n",
            "    }\n",
            "  ],\n",
            "  \"split\": \"train\",\n",
            "  \"summary\": {\n",
            "    \"total_score\": 6.0,\n",
            "    \"num_evaluated\": 6\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# List debug evaluation JSONs (most recent first)\n",
        "jsons = sorted(OUTPUT_JSON_DIR.glob(\"*evaluation*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "if jsons:\n",
        "    latest = jsons[0]\n",
        "    print(f\"Latest: {latest}\")\n",
        "    with open(latest) as f:\n",
        "        print(json.dumps(json.load(f), indent=2))\n",
        "else:\n",
        "    print(\"No evaluation JSONs found. Run the cell above first.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
